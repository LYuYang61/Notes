# 2、多无人机协同系统相关论文
### 1、Joint Trajectory and Scheduling Optimization  for Age of Synchronization Minimization in  UAV-Assisted Networks With Random Updates
- 论文地址：https://doi.org/10.1109/TCOMM.2023.3297198
#### 1.1 研究问题
- 本文考虑了在一个无人机辅助的网络，其中无人机在资源有限的传感器节点 ( SN ) 之间飞行并收集它们的状态更新【单架无人机】
- 本文通过**联合优化无人机的飞行轨迹和传感器节点的调度策略，来最小化同步年龄**（Age of Synchronization，AoS）【兼顾数据的“新鲜度”和“内容”】
#### 1.2 创新贡献
1. **引入全新指标：同步年龄（AoS）**
	- **AoI**：
		- 度量的是从数据包生成的那一刻起，到它被接收并处理的时间
		- 如果该任务没有新的数据包成功传输到基站，这个数据包的 AoI 会持续增长
		- 例如，一个传感器每秒生成一次数据，如果接收端已经 10 秒没有收到新数据，它的 AoI 就是 10 秒，无论这期间有没有新的“事件”发生
	- **AoS**：
		- 度量的是自接收到的最新数据包所携带的信息**不再是当前最新状态**以来所经过的时间
		- 如果传感器节点没有新的“事件”发生，即它的状态没有改变，那么即使没有新的数据传输，AoS 也保持为零
		- AoS 只有在接收端的数据状态和实际情况不同步时才会增长
	- 这种度量方式更适合事件驱动型（event-driven）的物联网应用，避免了 AoI 在无新事件发生时产生的“不必要成本”
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250924195204.png)
2. **创新的 DQN 算法框架**
	- 提出将**阈值调度 + DQN**（仅学习 UAV 运动决策）结合的实用算法，既保证调度近似最优，又降低 DQN 的搜索难度和收敛时间
	- 本文通过仿真（N=8 等设置）展示了在受限更新频率下，AoS 优化明显优于 AoI（Age-optimal）和“DQN-only”策略
3. **考虑了随机更新产生与有限更新频率约束**
	- 很多早期工作假设更新可随意触发或以固定方式生成
	- 本文将更新建模为 Poisson（速率 λn）并加入每个 SN 的最大更新频率 fn，增强了模型现实性（尤其适合能源受限的 SN）

### 2、Age of Information Minimization Using Multi-Agent  UAVs Based on AI-Enhanced Mean Field  Resource Allocation
- 论文地址：https://doi.org/10.1109/TVT.2024.3394235
#### 2.1 研究问题
- 如何在大规模多无人机（UAV）数据采集中，通过优化无人机的飞行轨迹和数据采集调度，来最小化数据的信息年龄（Age of Information, AoI）
- 本文将问题建模为**均值场博弈**（Mean Field Game, MFG），使用一种名为 MF-HPPO 的深度强化学习算法来求解，并利用**长短期记忆网络** ( long short term memory，LSTM ) 来预测时变的网络状态并稳定训练
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250925100121.png)
#### 2.2 创新贡献
##### 1、将问题建模为均值场博弈 MFG
- 传统方法（如微分博弈）会考虑每架无人机与其余所有无人机之间的相互作用。在无人机数量众多时，其复杂性会呈指数级增长
- MFG 假设每架无人机只关心它**自己的状态（如位置、AoI）以及其他无人机的均值场**【统计分布】
- 每架无人机都试图根据这个均值场来优化自己的策略（即飞行轨迹和数据调度），以最小化 AoI，同时，这个**均值场又是所有无人机个体策略的集合**
- 形成一个自洽的循环，直到达到一个**纳什均衡（Nash Equilibrium）**，此时每个无人机都选择了最优策略，并且没有任何一架无人机有动机单方面改变其策略
- **状态空间**：每架无人机的状态由其位置和当前维护的 AoI 组成
- **动作空间**：无人机的动作是混合的：
	- **连续动作**：无人机的速度和飞行方向，决定其轨迹
	- **离散动作**：是否选择与地面传感器进行通信，决定了数据采集的调度
- **成本函数**：
	- 无人机快速移动，会导致信道条件变差，重传次数增加，从而延长 AoI
	- 无人机慢速移动，由于数据采集不及时，可能会延长地面传感器的 AoI
	- 成本函数解决了这些权衡，并找到了平衡这些目标的最佳速度
- **均值场**：是所有无人机在空间中的**位置分布**
	- 本文使用 FPK 方程来描述这个分布如何随时间演化
	- 该方程能够捕捉无人机群在不同区域的密度变化，从而反映出它们集体行为的动态
##### 2、提出了混合动作空间的强化学习算法 MF-HPPO
- 由于直接在线求解 MFG 非常困难，特别是由于无人机之间无法瞬时得知彼此的巡航决策和 AoI 状态
- 本文选用 PPO（稳健的近端策略优化）+ 均值场近似 + 混合动作建模 + LSTM 状态表征的组合——即 MF-HPPO，使得每架 UAV 能在本地运行一个策略，既能输出连续控制，又能输出离散调度决策，同时把邻居/群体影响作为输入（均场信息）纳入决策
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250925152908.png)

- **核心模块**：
	1. **状态表征层 LSTM**：
		- 把时序的网络状态（本 UAV 的位置、历史 AoI、先前动作等）输入 LSTM，得到一个时间上下文的隐藏向量作为后续 actor/critic 的输入
		- 这样能捕捉 AoI 的时序相关性、航点序列依赖，稳定训练
	2. **混合动作，双 Actor**：
		- **连续 Actor**：输出巡航相关的连续控制量（如速度向量、目标坐标），在实现上用**多元正态分布**参数化
		- **离散 Actor**：输出选择哪个传感器的概率，用**分类分布**参数化（输出是对 J 个传感器的概率分布）
		- **混合策略的组合**：把连续策略和离散策略看作独立分布，然后**将两者相乘**得到混合动作概率
	3. **值网络 Critic**：
		- Critic 接收状态与**均场信息** $o_i$（邻居/群体行为的统计量）拼接后，输出一个标量 value，用于计算 critic loss
		- 均场信息作为额外输入，使 Critic 能估计“在该群体分布下”的价值
- **均值场近似**：
	- 在多智能体问题中计算全体联合动作的 Q 非常昂贵，本文采用邻域均值近似：把邻居的 Q 或动作信息取平均，作为本体决策的“均场指示器” $o_i$
- **策略优化核心 PPO-clip**：
	- PPO-clip 是一种一阶、稳健且易实现的近端策略优化方法，避免了 TRPO 的二阶复杂性，又能控制每次更新幅度，从而在连续与离散混合空间中提供稳定更新基础
	- 在 PPO 的目标函数里，分别考虑了连续部分和离散部分的概率，以及 critic loss，最后联合起来得到惩罚项
- **训练循环（直观的采样-更新流程）**：
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250926094935.png)

### 3、Toward Fault Tolerance in Multi-Agent  Reinforcement Learning
- 论文地址：https://doi.org/10.1109/TASE.2025.3592721
#### 3.1 研究问题
- **状态空间混乱问题**：当某个智能体发生故障时，它会提供异常或无效的观测信息，导致整个系统的状态空间变得混乱，使得其他健康的智能体难以从中提取出真正有用的信息来做出决策
- **样本不均衡问题**：在多智能体强化学习中，智能体在正常运行和故障发生这两种状态下产生的经验数据是极不均衡的。故障前的正常数据非常多，而故障后的关键数据相对稀少。如果使用传统的训练方法，模型会过度学习正常情况下的经验，导致对故障情况的学习效率低下，甚至无法有效地适应故障
- 本文通过**结合优化的模型架构和定制的训练数据采样策略来增强 MARL 的容错性**
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250926110745.png)
#### 3.2 创新贡献：AACFT 模型
- 在 **CTDE（集中训练、分布执行）** 框架下：
	- **Critic**：集中式，输入所有智能体的观测 + 动作，用来估计 Q 值
	- **Actor**：分布式，每个 agent 只看到自己的局部观测，输出自己的动作
- 问题：如果某个 agent 故障（观测缺失/为无效值），它的输入仍会进入 Critic/Actor，污染梯度，导致整体性能大幅下降
- 本文在网络内部加一个“自动过滤器”——注意力模块，让模型自己学会忽略故障 agent 的输入
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250926150829.png)
##### 1、注意力机制优化的网络架构
1. **改进的 Critic 网络**：重点是过滤坏信息
	- **输入处理**：
		- 对于正常 agent：输入 = 观测 $o_j$ +动作 $a_j$
		- 对于故障 agent：观测替换为一个**显式 flag 向量 z**（比如全 10），动作强制设为 0
		- 这样，网络能轻松区分“正常数据”和“坏数据”
	- **注意力模块的作用**：
		- 每个 agent 的输入先通过一个 embedding 层（MLP），得到低维表示
		- Critic 内部的 **attention** 对这些 embedding 分配权重（类似 Transformer 的机制）
		- 如果某个输入是“故障标记 z”，注意力权重会在训练过程中自动学到接近 0，相当于把这个 agent 的影响屏蔽掉
	- **结果**：Critic 在估计 Q 值时**只聚合有效 agent 的信息**，避免故障 agent 干扰
2. **改进的 Actor 网络**：重点是动态利用信息
	- Critic 是用来学习的，而 Actor 是执行时的策略网络，本文在 Actor 里也加了注意力，原因是：
		- 有时候“故障 agent 的最后一次信息”仍然有用（比如它最后的位置能帮助避障）
		- 有时候必须完全忽略它
	- **架构实现**：
		- 每个 Actor 的输入：本 agent 的局部观测，包括它自己看到的环境 + 它能接收到的其他 agent 的部分信息（可能包含故障标记 z）
		- 输入片段先经过 embedding，再进入一个**注意力模块**，模块里加了一个 **learnable token**（类似 Transformer 的 token），让网络能把多个信息片段聚合到一起
		- 注意力的输出就是 Actor 的**隐藏表示**（注意力模块输出的融合向量），然后进入 MLP 输出明确的动作
    - **结果**：actor 能根据任务需要**灵活决定该不该用其他 agent 的信息**，而不是被动全用
##### 2、为训练引入按模块的优先级经验回放（PER 扩展）
- 在多智能体系统里，故障通常发生在某个时刻：
	- **故障前 pre-fault**：经验很大量、很相似
	- **故障后 post-fault**：经验很稀少、很多样（需要多学，但在 buffer 里比例很小）
- 如果用传统**均匀采样**从经验池里取数据，训练大多浪费在重复的 pre-fault transition 上，而忽略了关键的 post-fault transition，导致模型很难学会在故障时该怎么做
- **核心思路**：按模块拆分 PER
	- Critic（全局价值网络）和每个 Actor（本地策略网络）各自有独立的“优先级经验队列”
	- 每个模块根据自己的 loss 表现来决定哪些样本更重要
	- 作用：
		- Critic 更新时重点学习那些对价值估计误差大的 transition
		- 某个 Actor 更新时重点学习那些对它的策略改进最有价值的 transition
	- **transition** 指的是环境交互过程中产生的一条**状态转移记录**，会包含所有智能体的信息
1. **Critic 的优先级回放**
	- 建立一个**共享的优先级队列 Qc**，所有 transition 都可以进来
	- Qc 里的优先级根据 Critic 的 TD 误差来算
	- 每次采样 Critic 的训练 batch，就从 Qc 里按优先级抽取
	- 这样 Critic 总是先学“最估不准的样本”，加速价值估计收敛
	- **TD 误差**：时序差分误差，本质是**当前网络估计的价值**和**实际经验得到的价值**之间的差距，如果误差很大，说明网络对这条经验的预测很不准，学习这条经验能改进很多
2. **Actor 的优先级回放**
	- 每个 Actor 都有一个**自己的优先队列 Qa, i**
	- Qa, i 里只放跟**该 actor** 有关的 transition（即该 agent 当时在场景里是正常的、没故障）
	- 优先级根据该 Actor 的 policy loss 来算
	- **关键点**：如果某个 transition 里 agent i 是故障的 → 就**不放进 Qa, i**，否则会误导它的策略
	- 这样每个 Actor 都能专注学习“自己还活跃时的经验”
3. **样本采样与训练**
	- 每次训练迭代：
		- Critic 从 Qc 采一批 → 更新 Q 网络
		- 每个 Actor 从自己的 Qa, i 采一批 → 更新自己的策略网络
	- 采样时不仅按优先级，还加了一个**重要性采样权重**来修正偏差，避免训练结果被某些高优先级样本过度主导

### 4、Energy-Efficient Multi-UAV Navigation for  Cooperative Data Sensing and Transmission
- 论文地址：https://doi.org/10.1109/TMC.2025.3612221
#### 4.1 研究问题
- 在部分可观测的环境下，如何设计一种能量高效的多无人机分布式导航策略，以协同完成数据感知和传输任务
- 本文核心目标是同时优化三个指标：
	- **最大化总数据收集量**：使无人机从地面 PoIs（传感器）收集并成功传输到基站的数据总量最大化
	- **确保地理公平性**：确保所有地面 PoIs 都能被公平地访问和覆盖，避免部分区域被忽略
	- **最小化能源消耗**：在满足任务需求的同时，尽可能减少无人机在整个服务周期内的总能量消耗，因为无人机机载能源有限
- 本文提出了一种**记忆增强的多智能体深度强化学习方法**，以确保在部分观测的情况下实现能量有效的分布式轨迹设计
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250927163441.png)
#### 4.2 创新贡献
##### 1、将问题建模为部分可观测马尔科夫决策过程 POMDP
- **现实场景**：每架 UAV 只能看到自己附近的环境，不能获得全局瞬时信息；同时决策需要面向长期（要考虑剩余电量、未来 PoI 的分布与上传机会）
- **目标是长期优化**：本文把每时隙上传的数据量/能耗与地理公平性结合长期 reward，鼓励既高效又公平的长期行为
1. **状态**：
	- 整个地图上每个 PoI 的剩余数据量（还需被采集的数据）、每架 UAV 的剩余能量/位置、每个格子或 PoI 的累计被访问次数等信息
	- 把这些用多通道栅格张量表示（Layer 1：PoI 剩余数据，Layer 2：UAV 能量分布，Layer 3：访问次数），让网络利用卷积结构提取空间特征更方便
2. **观测**：
	- 每架 UAV 在运行/部署时只能“看到”以自身为中心的局部窗口（受通信距离限制）
	- 观测空间由该窗口内的多通道张量构成（对应全局状态的局部裁剪）
3. **动作**：通常是连续动作，本文采用移动方向/距离的连续或离散化选择
4. **转移**：
	- 位置影响到 UAV 与 PoI /基站的信道条件（进而影响本时隙可上传的数据量）；上传部分消耗能量并减少 PoI 剩余数据
	- 环境按时隙推进，计算新的全局状态
	- 本文按离散时隙处理（移动阶段 + 采集/上传阶段）
5. **奖励**：
	- **外部奖励**：每个 UAV 在一个时隙内上传到地面基站的比特数（数据收益）除以该 UAV 在同一时隙消耗的能量，这个量反映“能效”
	- **地理公平性因子**：为了避免策略只集中在高密度或近基站的 PoI，论文把一个地理公平度 scalar（0~1）乘入 reward 中，公平度越高说明 PoI 间被访问/服务越均衡
	- **内在奖励**：为增强探索行为，论文引入 BeBold 类型的内在奖励——当 UAV 首次在本 episode 到达某个网格（或到达较少被访问的网格）时给额外正向奖励，使 UAV 更愿意覆盖未被访问区域，从而改善公平性与覆盖率
##### 2、提出一个专为部分可观测环境设计的框架 MEMDRL
- **框架整体结构**：MEMDRL 采用 **集中训练、分布执行** 的范式：
	- **训练阶段**：Critic 可见全局状态 + 所有 UAV 的动作，保证稳定学习
	- **执行阶段**：每个 UAV 只用自己的局部观测和历史信息（通过 ConvLSTM 编码），即可独立决策
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250927170640.png)

1. **空间-时间记忆模块 ConvLSTM Actor-Critic**
	- **问题**：UAV 只能看到局部窗口，单步信息不足；但轨迹是时序性的，PoI 数据采集/上传情况需要考虑过去的趋势
	- **设计**：在 Actor 和 Critic 网络中加入 **ConvLSTM**（卷积 LSTM）
	    - 卷积部分保留局部窗口的**空间结构**（比如相邻 PoI 的分布）
	    - LSTM 部分学习**时间依赖**（历史访问、能量消耗趋势）
	- **实现方式**：输入是多帧观测（比如最近 5 帧局部窗口），经过 ConvLSTM 提取时空特征，再送入全连接层生成动作（Actor）或 Q 值（Critic）
	- **好处**：
	    - Actor 能基于“最近几步的历史”来判断接下来要往哪里走
	    - Critic 在训练时更准确地评估长期价值
![image.png](https://qingwu-oss.oss-cn-heyuan.aliyuncs.com/lian/img/20250927175853.png)

2. **探索激励模块（BeBold 内在奖励）**
	- **问题**：多 UAV 可能贪心地反复访问同一片区域（因为那里的数据好采），导致覆盖不足、PoI 服务不均衡
	- **设计**：引入 **BeBold 内在奖励**
	    - 当 UAV 在某个 episode 中第一次访问某个格子时，就获得额外奖励
	    - 如果某个格子在历史上很少被访问，那么首次访问的奖励更大
	- **实现方式**：环境在每个 episode 内维护一个访问表（visited flag）
	    - 如果 UAV 第一次到达某格子 → 内在奖励 > 0
	    - 内在奖励值可用长期访问频率来调节：访问越稀少，奖励越高
	- **好处**：
	    - UAV 会主动去探索未覆盖区域 → 提升地理公平性
	    - 避免陷入局部最优（只服务高收益区域）
3. **优先经验回放模块 Multi-agent PER**
	- **问题**：在多智能体环境中，关键的经验很稀少（比如“几个 UAV 正好分工合作覆盖全图”），如果随机采样经验，可能效率低
	- **设计**：采用 PER，并根据所有 UAV 的 TD 误差之和来衡量每个经验的重要性
	    - TD 误差大 → 表示当前策略对该经验的估值不准确 → 更值得学习
	    - 所有 UAV 的 TD 误差加总 → 强调“团队协同”的关键经验
	- **实现方式**：
	    - 每条 transition 存进经验池时附带一个优先级
	    - 采样时按优先级的幂次分布来选取
	    - 更新时根据 TD 误差重新调整优先级
	- **好处**：
	    - 更快收敛
	    - 学习集中在“高价值、协同关键”的经验上
- **MEMDRL 的整体运行流程**：
	- **训练阶段**：
		1. **环境交互**：多 UAV 执行动作（基于 Actor + ConvLSTM），环境更新状态并计算奖励（外部 + 内在探索奖励）
		2. **存储经验**：把“全局状态、局部观测、联合动作、奖励、下一个状态“存进 PER buffer
		3. **采样学习**：
		    - 从经验池中优先采样 TD 误差大的 transition
		    - Critic 更新：用 ConvLSTM + 双 Critic 计算目标 Q 值，最小化 TD 误差
		    - Actor 更新：延迟更新，每隔几步用 Critic 的梯度指导 Actor 改进
		    - 更新 PER 优先级
		4. **目标网络更新**：软更新 Actor/Critic 的目标网络，保持训练稳定
	- **执行阶段**：
		- 部署时，每个 UAV 只用自己的**局部窗口观测历史**输入 Actor（ConvLSTM），输出移动动作
		- Critic 和全局状态只在训练时使用，不需要在实际执行阶段使用

### 5、Neighbor-Aware Multi-agent Deep  Reinforcement Learning for Decentralized  Dynamic Task Allocation in Multi-UAVs
- 论文地址：https://link.springer.com/chapter/10.1007/978-3-031-94892-3_13#citeas
#### 5.1 研究问题
- 本文关注的核心问题是：在不可预测的环境中，如何高效解决多无人机集群的“分散式动态任务分配”问题，特别是**针对随机到达且具有时延敏感性的协同任务**
- 本文提出**基于邻居感知的多智能体深度强化学习**（Neighbor-Aware MADRL），以邻居之间的局部信息交换代替全局知识，以制定鲁棒的、长期的分散任务分配策略

### 6、Joint AoI-Aware UAVs Trajectory Planning and  Data Collection in UAV-Based IoT Systems:  A Deep Reinforcement Learning Approach
- 论文地址：https://doi.org/10.1109/TCE.2024.3440406
#### 6.1 研究问题
- 本文旨在解决无人机辅助物联网（IoT）系统中，**多无人机协同轨迹规划和数据收集的联合优化问题**，其核心目标是最小化信息年龄（AoI）和 IoT 设备能耗的加权和
- 本文将问题转化为 MINLP 问题，并设计了一种**基于 DRL 的方法来确定无人机的航迹**
- 在此基础上，**利用辅助图中的最小代价二分匹配进行最优数据收集决策**，并进一步反馈给 DRL 智能体进行无人机轨迹规划

### 7、Joint Optimization of Data Acquisition and  Trajectory Planning for UAV-Assisted  Wireless Powered Internet of Things
- 论文地址：https://doi.org/10.1109/TMC.2024.3470831
#### 7.1 研究问题
- 本文主要针对**多无人机辅助无线供电物联网中，联合优化数据采集和轨迹规划的问题**
- 目标是：**最大化系统能量效率**（Energy Efficiency, EE） ，同时必须满足以下核心约束 ：
	- 无人机的移动性和飞行安全约束（如飞行范围、最小安全距离）
	- ISD（智能传感设备） 的服务质量（QoS） 约束（如数据上传能耗必须低于接收能量 ）
	- 任务完成时间约束（如总时延不超过最大服务期限）
- 本文将联合优化问题分成两个子问题：
	- **无人机轨迹规划**：提出了多智能体约束深度强化学习 ( Multi-agent Constrained Deep Reinforcement Learning，MCDRL ) 算法
	- **ISD-UAV 连接分配**：提出了一种基于匹配理论的低复杂度 ISD - UAV 连接分配算法